{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 2 - Introduction to Probability Theory\n",
    "\n",
    "> Probability theory is nothing but common sense reduced to calculation. P. Laplace (1812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Objectives\n",
    "+ To use probability theory to represent states of knowledge.\n",
    "+ To use probability theory to extend Aristotelian logic to reason under uncertainty.\n",
    "+ To learn about the **pruduct rule** of probability theory.\n",
    "+ To learn about the **sum rule** of probability theory.\n",
    "+ What is a **random variable**?\n",
    "+ What is a **discrete random variable**?\n",
    "+ When are two random variables **independent**?\n",
    "+ What is a **continuous random variable**?\n",
    "+ What is the **cumulative distribution function**?\n",
    "+ What is the **probability density function**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Readings\n",
    "\n",
    "Before coming to class, please read the following:\n",
    "\n",
    "+ [Chapter 1 of Probabilistic Programming and Bayesian Methods for Hackers](http://nbviewer.ipython.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Chapter1.ipynb)\n",
    "+ [Chapter 1](http://home.fnal.gov/~paterno/images/jaynesbook/cc01p.pdf) of (Jaynes, 2003).\n",
    "+ [Chapter 2](http://home.fnal.gov/~paterno/images/jaynesbook/cc02p.pdf) of (Jaynes, 2003) (skim through)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## The basic desiderata of probability theory\n",
    "It is actually possible to derive the rules of probability based on a system of common sense requirements.\n",
    "Paraphrasing \n",
    "[Chapter 1](http://home.fnal.gov/~paterno/images/jaynesbook/cc01p.pdf) of \\cite{jaynes2003}),\n",
    "we would like our system to satisfy the following desiderata:\n",
    "\n",
    "1) *Degrees of plausibility are represented by real numbers.*\n",
    "\n",
    "2) *The system should have a qualitative correspondance with common sense.*\n",
    "\n",
    "3) *The system should be consistent in the sense that:*\n",
    "    \n",
    "   + *If a conclusion can be reasoned out in more than one way, then every possible way must lead to the same result.*\n",
    "    \n",
    "   + *All the evidence relevant to a question should be taken into account.*\n",
    "    \n",
    "   + *Equivalent states of knowledge must be represented by equivalent plausibility assignments.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to speak about probabilities?\n",
    "Let\n",
    "+ A be a logical sentence,\n",
    "+ B be another logical sentence, and\n",
    "+ and I be all other information we know.\n",
    "\n",
    "There is no restriction on what A and B may be as soon as none of them is a contradiction.\n",
    "We write as a shortcut:\n",
    "$$\n",
    "\\mbox{not A} \\equiv \\neg A,\n",
    "$$\n",
    "$$\n",
    "A\\;\\mbox{and}\\;B \\equiv A,B \\equiv AB,\n",
    "$$\n",
    "$$\n",
    "A\\;\\mbox{or}\\;B \\equiv A + B.\n",
    "$$\n",
    "\n",
    "We **write**:\n",
    "$$\n",
    "p(A|BI),\n",
    "$$\n",
    "and we **read**:\n",
    "> the probability of A being true given that we know that B and I are true\n",
    "\n",
    "or (assuming knowledge I is implied)\n",
    "\n",
    "> the probability of A being true given that we know that B is true\n",
    "\n",
    "or (making it even shorter)\n",
    "\n",
    "> the probability of A given B.\n",
    "\n",
    "$$\n",
    "p(\\mbox{something} | \\mbox{everything known}) = \\mbox{probability something is true conditioned on what is known}.\n",
    "$$\n",
    "\n",
    "$p(A|B,I)$ is just a number between 0 and 1 that corresponds to the degree of plaussibility of A conditioned on B and I.\n",
    "0 and 1 are special.\n",
    "\n",
    "+ If\n",
    "$$\n",
    "p(A|BI) = 0,\n",
    "$$\n",
    "we say that we are certain that A is false if B is true.\n",
    "\n",
    "+ If\n",
    "$$\n",
    "p(A|BI) = 1,\n",
    "$$\n",
    "we say that we are certain that A is false if B is false.\n",
    "\n",
    "+ If\n",
    "$$\n",
    "p(A|BI) \\in (0, 1),\n",
    "$$\n",
    "we say that we are uncertain about A given that B is false.\n",
    "Depending on whether $p(A|B,I)$ is closer to 0 or 1 we beleive more on one possibiliy or another.\n",
    "Complete ignorance corresponds to a probability of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rules of probability theory\n",
    "\n",
    "According to\n",
    "[Chapter 2](http://home.fnal.gov/~paterno/images/jaynesbook/cc02m.pdf) of \\cite{jaynes2003} the desiderata are enough\n",
    "to derive the rules of probability.\n",
    "These rules are:\n",
    "\n",
    "+ The **obvious rule** (in lack of a better name):\n",
    "$$\n",
    "p(A | I) + p(\\neg A | I) = 1.\n",
    "$$\n",
    "\n",
    "+ The **product rule** (also known as the Bayes rule or Bayes theorem):\n",
    "$$\n",
    "p(AB|I) = p(A|BI)p(B|I).\n",
    "$$\n",
    "or\n",
    "$$\n",
    "p(AB|I) = p(B|AI)p(A|I).\n",
    "$$\n",
    "\n",
    "These two rules are enough to compute any probability we want. Let us demonstrate this by a very simple example.\n",
    "\n",
    "### Example: Drawing balls from a box without replacement\n",
    "Consider the following example of prior information I:\n",
    "\n",
    "> We are given a box with 10 balls 6 of which are red and 4 of which are blue.\n",
    "The box is sufficiently mixed so that when we get a ball from it, we don't know which one we pick.\n",
    "When we take a ball out of the box, we do not put it back.\n",
    "\n",
    "![](../slides/urn.png)\n",
    "\n",
    "Let A be the sentence:\n",
    "\n",
    "> The first ball we draw is blue.\n",
    "\n",
    "Intuitively, we would set the probability of A equal to:\n",
    "$$\n",
    "p(A|I) = \\frac{4}{10}.\n",
    "$$\n",
    "\n",
    "This choice can actually be justified, but we will come to this later in this course.\n",
    "From the \"obvious rule\", we get that the probability of not drawing a blue ball, i.e.,\n",
    "the probability of drawing a red ball in the first draw is:\n",
    "$$\n",
    "p(\\neg A|I) = 1 - p(A|I) = 1 - \\frac{4}{10} = \\frac{6}{10}.\n",
    "$$\n",
    "\n",
    "Now, let B be the sentence:\n",
    "\n",
    "> The second ball we draw is red.\n",
    "\n",
    "What is the probability that we draw a red ball in the second draw given that we drew a blue ball in the first draw?\n",
    "Just before our second draw, there remain 9 bals in the box, 3 of which are blue and 6 of which are red.\n",
    "Therefore:\n",
    "$$\n",
    "p(B|AI) = \\frac{6}{9}.\n",
    "$$\n",
    "\n",
    "We have not used the product rule just yet. What if we wanted to find the probability that we draw a blue during the first draw and a red during the second draw? Then,\n",
    "$$\n",
    "p(AB|I) = p(A|I)p(B|AI) = \\frac{4}{10}\\frac{6}{9} = \\frac{24}{90}.\n",
    "$$\n",
    "\n",
    "What about the probability o a red followed by a blue? Then,\n",
    "$$\n",
    "p(\\neg AB|I) = p(\\neg A|I)p(B|AI) = \\left[1 - p(A|I) \\right]p(B|\\neg AI) = \\frac{6}{10}\\frac{5}{9} = \\frac{30}{90}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other rules of probability theory\n",
    "\n",
    "All the other rules of probability theory can be derived from these two rules.\n",
    "To demonstrate this, let's prove that:\n",
    "$$\n",
    "p(A + B|I) = p(A|I) + p(B|I) - p(AB|I).\n",
    "$$\n",
    "Here we go:\n",
    "\\begin{eqnarray*}\n",
    "p(A+B|I) &=& 1 - p(\\neg A \\neg B|I)\\;\\mbox{(obvious rule)}\\\\\n",
    "         &=& 1 - p(\\neg A|\\neg BI)p(\\neg B|I)\\;\\mbox{(product rule)}\\\\\n",
    "         &=& 1 - [1 - p(A |\\neg BI)]p(\\neg B|I)\\;\\mbox{(obvious rule)}\\\\\n",
    "         &=& 1 - p(\\neg B|I) + p(A|\\neg B I)p(\\neg B|I)\\\\\n",
    "         &=& 1 - [1 - p(B|I)] + p(A|\\neg B I)p(\\neg B|I)\\\\\n",
    "         &=& p(B|I) + p(A|\\neg B I)p(\\neg B|I)\\\\\n",
    "         &=& p(B|I) + p(A\\neg B|I)\\\\\n",
    "         &=& p(B|I) + p(\\neg B|AI)p(A|I)\\\\\n",
    "         &=& p(B|I) + [1 - p(B|AI)] p(A|I)\\\\\n",
    "         &=& p(B|I) + p(A|I) - p(B|AI)p(A|I)\\\\\n",
    "         &=& p(A|I) + p(B|I) - p(AB|I).\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sum rule\n",
    "Now consider a finite set of logical sentences, $B_1,\\dots,B_n$ such that:\n",
    "1. One of them is definitely true:\n",
    "    $$\n",
    "    p(B_1+\\dots+B_n|I) = 1.\n",
    "    $$\n",
    "2. They are mutually exclusive:\n",
    "    $$\n",
    "    p(B_iB_j|I) = 0,\\;\\mbox{if}\\;i\\not=j.\n",
    "    $$\n",
    "\n",
    "The **sum rule** states that:\n",
    "    $$\n",
    "    P(A|I) = \\sum_i p(AB_i|I) = \\sum_i p(A|B_i I)p(B_i|I).\n",
    "    $$\n",
    "We can prove this by induction, but let's just prove it for $n=2$:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "p(A|I) &=& p[A(B_1+B_2)|I]\\\\\n",
    "       &=& p(AB_1 + AB_2|I)\\\\\n",
    "       &=& p(AB_1|I) + p(AB_2|I) - p(AB_1B_2|I)\\\\\n",
    "       &=& p(AB_1|I) + p(AB_2|I),\n",
    "\\end{array}\n",
    "$$\n",
    "since\n",
    "$$\n",
    "p(AB_1B_2|I) = p(A|B_1B_2|I)p(B_1B_2|I) = 0.\n",
    "$$\n",
    "\n",
    "Let's go back to our example. We can use the sum rule to compute the probability of getting a red ball on the second draw independently of what we drew first. This is how it goes:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "p(B|I) &=& p(AB|I) + p(\\neg AB|I)\\\\\n",
    "       &=& p(B|AI)p(A|I) + p(B|\\neg AI) p(\\neg A|I)\\\\\n",
    "       &=& \\frac{6}{9}\\frac{4}{10} + \\frac{5}{9}\\frac{6}{10}\\\\\n",
    "       &=& \\dots\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Medical Diagnosis\n",
    "\n",
    "This example is a modified version of the one found in [Lecture 1](http://www.zabaras.com/Courses/BayesianComputing/IntroToProbabilityAndStatistics.pdf) of the Bayesian Scientific Computing course offered during Spring 2013 by Prof. N. Zabaras at Cornell University.\n",
    "\n",
    "We are going to examine the usefullness of a new tuberculosis test. Let the prior information, I, be:\n",
    "\n",
    "> The percentage of the population infected by tuberculosis is 0.4%. We have run several experiments and determined that:\n",
    " + If a tested patient has the disease, then 80% of the time the test comes out positive.\n",
    " + If a tested patient does not have the disease, then 90% of the time, the test comes out negative.\n",
    " \n",
    "Suppose now that you administer this test to a patient and that the result is positive. How confident are you that the patient does indeed have the disease?\n",
    "\n",
    "Let's use probability theory to answer this question. Let A be the event:\n",
    "> The patient's test is positive.\n",
    "\n",
    "Let B be the event:\n",
    "> The patient has tuberculosis.\n",
    "\n",
    "According to the prior information, we have:\n",
    "$$\n",
    "p(B|I) = p(\\mbox{has tuberculosis}|I) = 0.004,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(A|B,I) = p(\\mbox{test is positive}|\\mbox{has tuberculosis},I) = 0.8.\n",
    "$$\n",
    "Similarly,\n",
    "$$\n",
    "p(A|\\neg B, I) = p(\\mbox{test is positive}|\\mbox{does not have tuberculosis}, I) = 0.1.\n",
    "$$\n",
    "We are looking for:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "p(\\mbox{has tuberculosis}|\\mbox{test is positive},I) &=& P(B|A,I)\\\\\n",
    "&=& \\frac{p(AB|I)}{p(A|I)} \\\\\n",
    "&=& \\frac{p(A|B,I)p(B|I)}{p(A|B,I)p(B|I) + p(A|\\neg B, I)p(\\neg B|I)}\\\\\n",
    "&=& \\frac{0.8\\times 0.004}{0.8\\times 0.004 + 0.1 \\times 0.996}\\\\\n",
    "&\\approx& 0.031.\n",
    "\\end{array}\n",
    "$$\n",
    "How much would you pay for such a test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Independence\n",
    "\n",
    "We say that $A$ and $B$ are **independent** (conditional on I), and write,\n",
    "$$\n",
    "A\\perp B|I,\n",
    "$$\n",
    "if knowledge of one does not yield any information about the other. Mathematically, by $A\\perp B|I$, we mean that:\n",
    "$$\n",
    "p(A|B,I) = p(A|I).\n",
    "$$\n",
    "Using the product rule, we can easily show that:\n",
    "$$\n",
    "A\\perp B|I \\iff p(AB|I) = p(A|I)p(B|I).\n",
    "$$\n",
    "\n",
    "### Question\n",
    "+ Give an example of $I, A$ and $B$ so that $A\\perp B|I$.\n",
    "\n",
    "Now, let $C$ be another event. We say that $A$ and $B$ are **independent** conditional on $C$ (and I), and write:\n",
    "$$\n",
    "A\\perp B|C,I,\n",
    "$$\n",
    "if knowlege of $C$ makes information about $A$ irrelevant to $B$ (and vice versa). Mathematically, we mean that:\n",
    "$$\n",
    "p(A|B,C,I) = p(A|C,I).\n",
    "$$\n",
    "\n",
    "### Question\n",
    "+ Give an example of $I,A,B,C$ so that $A\\perp B|C,I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n",
    "\n",
    "The formal mathematical definition of a random variable involves measure theory and is well beyond the scope of this course.\n",
    "Fortunately, we do not have to go through that route to get a theory that is useful in applications.\n",
    "For us, a **random variable** $X$ will just be a variable of our problem whose value is unknown to us.\n",
    "Note that, you should not take the word \"random\" too literally.\n",
    "If we could, we would change the name to **uncertain** or **unknown** variable.\n",
    "A random variable could correspond to something fixed but unknown, e.g., the number of balls in a box,\n",
    "or it could correspond to something truely random, e.g., the number of particles that hit a [Geiger counter](https://en.wikipedia.org/wiki/Geiger_counter) in a specific time interval.\n",
    "\n",
    "### Discrete Random Variables\n",
    "We say that a random variable $X$ is discrete, if the possible values it can take are discrete (possibly countably infinite).\n",
    "We write:\n",
    "$$\n",
    "p(X = x|I) \n",
    "$$\n",
    "and we read \"the probability of $X$ being $x$\".\n",
    "If it does not cause any ambiguity, sometimes we will simplify the notation to:\n",
    "$$\n",
    "p(x) \\equiv p(X=x|I).\n",
    "$$\n",
    "Note that $p(X=x)$ is actually a discrete function of $x$ which depends on our beliefs about $X$.\n",
    "The function $p(x) = p(X=x|I)$ is known as the probability density function of $X$.\n",
    "\n",
    "Now let $Y$ be another random variable. \n",
    "The **sum rule** becomes:\n",
    "$$\n",
    "p(X=x|I) = \\sum_{y}p(X=x,Y=y|I) = \\sum_y p(X=x|Y=y,I)p(Y=y|I),\n",
    "$$\n",
    "or in simpler notation:\n",
    "$$\n",
    "p(x) = \\sum_y p(x,y) = \\sum_y p(x|y)p(y).\n",
    "$$\n",
    "The function $p(X=x, Y=y|I) \\equiv p(x, y)$ is known as the joint *probability mass function* of $X$ and $Y$.\n",
    "\n",
    "The **product rule** becomes:\n",
    "$$\n",
    "p(X=x,Y=y|I) = p(X=x|Y=y,I)p(Y=y|I),\n",
    "$$\n",
    "or in simpler notation:\n",
    "$$\n",
    "p(x,y) = p(x|y)p(y).\n",
    "$$\n",
    "\n",
    "We say that $X$ and $Y$ are **independent** and write:\n",
    "$$\n",
    "X\\perp Y|I,\n",
    "$$\n",
    "if knowledge of one does not yield any information about the other.\n",
    "Mathematically, $Y$ gives no information about $X$ if:\n",
    "$$\n",
    "p(x|y) = p(x).\n",
    "$$\n",
    "From the product rule, however, we get that:\n",
    "$$\n",
    "p(x) = p(x|y) = \\frac{p(x,y)}{p(y)},\n",
    "$$\n",
    "from which we see that the joint distribution of $X$ and $Y$ must factorize as:\n",
    "$$\n",
    "p(x, y) = p(x) p(y).\n",
    "$$\n",
    "It is trivial to show that if this factorization holds, then\n",
    "$$\n",
    "p(y|x) = p(y),\n",
    "$$\n",
    "and thus $X$ yields no information about $Y$ either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Random Variables\n",
    "A random variable $X$ is continuous if the possible values it can take are continuous. The probability of a continuous variable getting a specific value is always zero. Therefore, we cannot work directly with probability mass functions as we did for discrete random variables. We would have to introduce the concepts of the **cumulative distribution function** and the **probability density function**. Fortunately, with the right choice of mathematical symbols, the theory will look exactly the same.\n",
    "\n",
    "Let us start with a real continuous random variable $X$, i.e., a random variable taking values in the real line $\\mathbb{R}$. Let $x \\in\\mathbb{R}$ and consider the probability of $X$ being less than or equal to $x$:\n",
    "$$\n",
    "F(x) := p(X\\le x|I).\n",
    "$$\n",
    "$F(x)$ is known as the **cumulative distribution function** (CDF). Here are some properties of the CDF whose proof is\n",
    "left as an excersise:\n",
    "\n",
    "+ The CDF starts at zero and goes up to one:\n",
    "$$\n",
    "F(-\\infty) = 0\\;\\mbox{and}\\;F(+\\infty) = 1.\n",
    "$$\n",
    "\n",
    "+ $F(x)$ is an increasing function of $x$, i.e.,\n",
    "$$\n",
    "x_1 \\le x_2 \\implies F(x_1)\\le F(x_2).\n",
    "$$\n",
    "\n",
    "+ The probability of $X$ being in the interval $[x_1,x_2]$ is:\n",
    "$$\n",
    "p(x_1 \\le X \\le x_2|I) = F(x_2) - F(x_1).\n",
    "$$\n",
    "\n",
    "Now, assume that the derivative of $F(x)$ with respect to $x$ exists.\n",
    "Let us call it $f(x)$:\n",
    "$$\n",
    "f(x) = \\frac{dF(x)}{dx}.\n",
    "$$\n",
    "Using the fundamental theorem of calculus, it is trivial to show Eq. (\\ref{eq:CDF_prob}) implies:\n",
    "\\begin{equation}\n",
    "p(x_1 \\le X \\le x_2|I) = \\int_{x_1}^{x_2}f(x)dx.\n",
    "\\end{equation}\n",
    "$f(x)$ is known as the **probability density function** (PDF) and it is measured in probability per unit of $X$.\n",
    "To see this note that:\n",
    "$$\n",
    "p(x \\le X \\le x + \\delta x|I) = \\int_{x}^{x+\\delta x}f(x')dx' \\approx f(x)\\delta x,\n",
    "$$\n",
    "so that:\n",
    "$$\n",
    "f(x) \\approx \\frac{p(x \\le X \\le x + \\delta x|I)}{\\delta x}.\n",
    "$$\n",
    "\n",
    "The PDF should satisfy the following properties:\n",
    "\n",
    "+ It should be positive\n",
    "$$\n",
    "f(x) \\ge 0,\n",
    "$$\n",
    "+ It should integrate to one:\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} f(x) dx = 1.\n",
    "$$\n",
    "\n",
    "#### Notation about the PDF of continuous random variables\n",
    "In order to make all the formulas of probability theory the same, we define for a continuous random variable $X$:\n",
    "$$\n",
    "p(x) := f(x) = \\frac{dF(x)}{dx} = \\frac{d}{dx}p(X \\le x|I).\n",
    "$$\n",
    "But keep in mind, that if $X$ is continuous $p(x)$ is not a probability but a probability density.\n",
    "That is, it needs a $dx$ to become a probability.\n",
    "\n",
    "Let the PDF $p(x)$ of $X$ and the PDF $p(y)$ of $Y$ ($Y$ is another continuous random variable).\n",
    "We can find the PDF of the random variable $X$ conditioned on $Y$, i.e., the PDF of $X$ if $Y$ is directly observed.\n",
    "This is the **product rule** for continuous random variables:\n",
    "\\begin{equation}\n",
    "\\label{eq:continuous_bayes}\n",
    "p(y|x) = \\frac{p(x, y)}{p(y)},\n",
    "\\end{equation}\n",
    "where $p(x,y)$ is the **joint PDF** of $X$ and $Y$.\n",
    "The **sum rule** for continous random variables is:\n",
    "\\begin{equation}\n",
    "\\label{eq:continuous_sum}\n",
    "p(x) = \\int p(x, y) dy = \\int p(x | y) p(y) dy.\n",
    "\\end{equation}\n",
    "\n",
    "The similarity between these rules and the discrete ones is obvious.\n",
    "We have prepared a table to help you remember it.\n",
    "\n",
    "| Concept | Discrete Random Variables | Continuous Random Variables |\n",
    "|---|---------------|-----------------|\n",
    "|$p(x)$| in units of robability | in units of probability per unit of $X$|\n",
    "|sum rule| $\\sum_y p(x,y) = \\sum_y p(x|y)p(y)$ | $\\int_y p(x,y) dy = \\int_y p(x|y) p(y)$| \n",
    "|product rule| $p(x,y) = p(x|y)p(y)$ | $p(x,y) = p(x|y)p(y)$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectations\n",
    "\n",
    "Let $X$ be a random variable. The expectation of $X$ is defined to be:\n",
    "$$\n",
    "\\mathbb{E}[X] := \\mathbb{E}[X | I] = \\int x p(x) dx.\n",
    "$$\n",
    "Now let $g(x)$ be any function. The expectation of $g(X)$, i.e., the random variable defined after passing $X$ through $g(\\cdot)$, is:\n",
    "$$\n",
    "\\mathbb{E}[g(X)] := \\mathbb{E}[g(X)|I] = \\int g(x)p(x)dx.\n",
    "$$\n",
    "As usual, calling $\\mathbb{E}[\\cdot]$ is not a very good name.\n",
    "You may think of $\\mathbb{E}[g(X)]$ as the expected value of $g(X)$, but do not take it too far.\n",
    "Can you think of an example in which the expected value is never actually observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Expectation\n",
    "Let $X$ and $Y$ be two random variables. The conditional expectation of $X$ given $Y=y$ is defined to be:\n",
    "$$\n",
    "\\mathbb{E}[X|Y=y] := \\mathbb{E}[X|Y=y,I] = \\int xp(x|y)dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Expectations\n",
    "\n",
    "The following properties of expectations of random variables are extremely helpful. In what follows, $X$ and $Y$ are random variables and $c$ is a constant:\n",
    "\n",
    "+ Sum of random variable with a constant:\n",
    "$$\n",
    "\\mathbb{E}[X+c] = \\mathbb{E}[X] + c.\n",
    "$$ \n",
    "\n",
    "+ Sum of two random variables:\n",
    "$$\n",
    "\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y].\n",
    "$$\n",
    "\n",
    "+ Product of random variable with constant:\n",
    "$$\n",
    "\\mathbb{E}[cX] = c\\mathbb{E}[X].\n",
    "$$\n",
    "\n",
    "+ If $X\\perp Y$, then:\n",
    "$$\n",
    "\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y].\n",
    "$$\n",
    "**NOTE**: This property does not hold if $X$ and $Y$ are not independent!\n",
    "\n",
    "+ If $f(\\cdot)$ is a convex function, then:\n",
    "$$\n",
    "f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)].\n",
    "$$\n",
    "**NOTE**: The equality holds only if $f(\\cdot)$ is linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of a Random Variable\n",
    "\n",
    "The variance of $X$ is defined to be:\n",
    "$$\n",
    "\\mathbb{V}[X] = \\mathbb{E}\\left[X - \\mathbb{E}[X])^2\\right].\n",
    "$$\n",
    "It is easy to prove (and a very useful formulat to remember), that:\n",
    "$$\n",
    "\\mathbb{V}[X] = \\mathbb{E}[X^2] - \\left(\\mathbb{E}[X]\\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance of Two Random Variables\n",
    "Let $X$ and $Y$ be two random variables.\n",
    "The covariance between $X$ and $Y$ is defined to be:\n",
    "$$\n",
    "\\mathbb{C}[X, Y] = \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)\n",
    "\\left(Y-\\mathbb{E}[Y]\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the Variance\n",
    "Let $X$ and $Y$ be random variables and $c$ be a constant.\n",
    "Then:\n",
    "+ Sum of random variable with a constant:\n",
    "$$\n",
    "\\mathbb{V}[X + c] = \\mathbb{V}[X].\n",
    "$$\n",
    "+ Product of random variable with a constant:\n",
    "$$\n",
    "\\mathbb{V}[cX] = c^2\\mathbb{V}[X].\n",
    "$$\n",
    "+ Sum of two random variables:\n",
    "$$\n",
    "\\mathbb{V}[X+Y] = \\mathbb{V}[X] + \\mathbb{V}[Y] + 2\\mathbb{C}(X,Y).\n",
    "$$\n",
    "+ Sum of two independent random variables:\n",
    "$$\n",
    "\\mathbb{V}[X+Y] = \\mathbb{V}[X] + \\mathbb{V}[Y].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(<a id=\"cit-jaynes2003\" href=\"#call-jaynes2003\">Jaynes, 2003</a>) E T Jaynes, ``_Probability Theory: The Logic of Science_'',  2003.  [online](http://bayes.wustl.edu/etj/prob/book.pdf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
